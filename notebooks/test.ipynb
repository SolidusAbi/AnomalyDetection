{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "ae_dir = os.path.join(project_dir, 'modules/AutoEncoder')\n",
    "if ae_dir not in sys.path:\n",
    "    sys.path.append(ae_dir)\n",
    "\n",
    "ipdl_dir = os.path.join(project_dir, 'modules/IPDL')\n",
    "if ipdl_dir not in sys.path:\n",
    "    sys.path.append(ipdl_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from autoencoder import SDAE, SDAE_TYPE\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial AutoEncoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "sdae = SDAE([28*28, 1024, 1024, z_dim], SDAE_TYPE.linear, dropout=True, \n",
    "            activation_func=[nn.ReLU(), nn.ReLU(), nn.Identity()])\n",
    "\n",
    "sdae = sdae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=True):\n",
    "        r'''\n",
    "            Parameters\n",
    "            ----------\n",
    "                input_dim: int\n",
    "                    Indicate the dimensionality of latent space Z\n",
    "\n",
    "                hidden_dim: int\n",
    "        '''\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            *( nn.Dropout(.2), nn.ReLU(inplace=True) ) if dropout else ( nn.ReLU(), ),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            *( nn.Dropout(.2), nn.ReLU(inplace=True) ) if dropout else ( nn.ReLU(), ),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )  \n",
    "    def forward(self, x):\n",
    "        return self.D(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "Q = sdae.encode.to(device)\n",
    "\n",
    "# Decoder\n",
    "P = sdae.decode.to(device)\n",
    "\n",
    "# Discriminator\n",
    "D = Discriminator(z_dim, 512, dropout=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "dataset_full = MNIST('data/', train = True, download = True, transform = transforms.ToTensor())\n",
    "# idx = torch.where((dataset_full.targets == 0) | (dataset_full.targets == 2))[0]\n",
    "normal_idx = torch.where((dataset_full.targets == 0))[0]\n",
    "anomaly_idx = torch.where((dataset_full.targets == 2))[0]\n",
    "idx = torch.cat([normal_idx, anomaly_idx[:512]])\n",
    "\n",
    "x_train_set = Subset(dataset_full, idx)\n",
    "train_loader =  DataLoader(x_train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO REMOVE! Just testing!\n",
    "x, y = next(iter(train_loader))\n",
    "plt.imshow(x[0,0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_full = MNIST('data/', train=False, download=True, transform = transforms.ToTensor())\n",
    "idx_0 = torch.where(test_dataset_full.targets == 0)[0]\n",
    "idx_2 = torch.where(test_dataset_full.targets == 2)[0]\n",
    "\n",
    "test_dataset_0 = Subset(test_dataset_full, idx_0)\n",
    "test_dataset_2 = Subset(test_dataset_full, idx_2)\n",
    "test_loader_0 =  DataLoader(test_dataset_0, 512, shuffle=False)\n",
    "test_loader_2 =  DataLoader(test_dataset_2, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining 2-Dimensional Guassian as prior\n",
    "$p(z) \\sim \\mathcal{N}_n(\\mu, \\sum)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = MultivariateNormal(loc=torch.zeros(z_dim), covariance_matrix=torch.eye(z_dim))\n",
    "\n",
    "plt.imshow(prior.sample((128,)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rates\n",
    "gen_lr = 1e-3\n",
    "reg_lr = 5e-4\n",
    "\n",
    "#encode/decode optimizers\n",
    "# optim_P = torch.optim.Adam(P.parameters(), lr=gen_lr)\n",
    "# optim_Q_enc = torch.optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "optim_sdae = torch.optim.Adam(sdae.parameters(), lr=gen_lr)\n",
    "\n",
    "#regularizing optimizers\n",
    "optim_Q_gen = torch.optim.Adam(Q.parameters(), lr=reg_lr)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 500\n",
    "recons_criterion = nn.MSELoss()\n",
    "# recons_criterion = nn.BCELoss()\n",
    "eps = 1e-12\n",
    "tb_writer = SummaryWriter('log/AAE')\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    recons_loss_ = []\n",
    "    disc_loss_ = []\n",
    "    gen_loss_ = []\n",
    "    Q.train()\n",
    "    P.train()\n",
    "    for idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.flatten(1)\n",
    "        # optim_P.zero_grad()\n",
    "        # optim_Q_enc.zero_grad()\n",
    "        optim_sdae.zero_grad()\n",
    "        optim_Q_gen.zero_grad()\n",
    "        optim_D.zero_grad()\n",
    "\n",
    "        # Reconstruction loss\n",
    "        # z = Q(inputs)\n",
    "        # out = P(z)\n",
    "        out = sdae(inputs)\n",
    "\n",
    "        recon_loss = recons_criterion(out, inputs)\n",
    "        recons_loss_.append(recon_loss.item())\n",
    "\n",
    "        recon_loss.backward()\n",
    "        optim_sdae.step()\n",
    "        # optim_P.step()\n",
    "        # optim_Q_enc.step()\n",
    "\n",
    "        # Discriminator\n",
    "        # Prior as an multivariate gaussian function $p(z) \\sim \\mathcal{N}_n(\\mu, \\sum)$\n",
    "        # this is constraining the Z-projection to be normal!\n",
    "        Q.eval()\n",
    "        z_real_gauss = prior.sample((inputs.size(0),)).to(device)\n",
    "        D_real_gauss = D(z_real_gauss)\n",
    "\n",
    "        z_fake_gauss = Q(inputs)\n",
    "        D_fake_gauss = D(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + eps) + torch.log(1 - D_fake_gauss + eps))\n",
    "        disc_loss_.append(D_loss.item())\n",
    "\n",
    "        D_loss.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # Generator\n",
    "        Q.train()\n",
    "        z_fake_gauss = Q(inputs)\n",
    "        D_fake_gauss = D(z_fake_gauss)\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + eps))\n",
    "        gen_loss_.append(G_loss.item())\n",
    "\n",
    "        G_loss.backward()\n",
    "        optim_Q_gen.step()\n",
    "\n",
    "    tb_writer.add_scalar('train/reconstruction', np.array(recons_loss_).mean(), epoch)\n",
    "    tb_writer.add_scalar('train/Discrimator', np.array(disc_loss_).mean(), epoch)\n",
    "    tb_writer.add_scalar('train/Generator', np.array(gen_loss_).mean(), epoch)\n",
    "    \n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_0 = []\n",
    "        loss_2 = []\n",
    "        for inputs, _ in test_loader_0:\n",
    "            inputs = inputs.to(device).flatten(1)\n",
    "            z = Q(inputs)\n",
    "            out = P(z)\n",
    "            \n",
    "            loss_0.append(recons_criterion(out, inputs).item())\n",
    "\n",
    "        for inputs, _ in test_loader_2:\n",
    "            inputs = inputs.to(device).flatten(1)\n",
    "            z = Q(inputs)\n",
    "            out = P(z)\n",
    "            \n",
    "            loss_2.append(recons_criterion(out, inputs).item())\n",
    "        \n",
    "\n",
    "        tb_writer.add_scalar('test/reconstruction/0', np.array(loss_0).mean(), epoch)\n",
    "        tb_writer.add_scalar('test/reconstruction/2',np.array(loss_2).mean(), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Q(inputs)\n",
    "out = P(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(inputs)-1)\n",
    "test_in = inputs[idx].reshape(28,28)\n",
    "test_out = out[idx].reshape(28,28)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_out.cpu().detach())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_in.cpu().detach())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "loader = DataLoader(x_train_set, 2048, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_density_and_recon_error(loader:DataLoader, Q:nn.Module, P:nn.Module, kde: KernelDensity) -> tuple:\n",
    "    density = None\n",
    "    recon_error_list = []\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    for inputs, _ in loader:\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "        z = Q(inputs)\n",
    "        reconstruction = P(z)\n",
    "        recon_error_list.append(recon_criterion(reconstruction, inputs).cpu().item())\n",
    "\n",
    "        if density is None:\n",
    "            density = kde.score_samples(inputs.cpu())\n",
    "        else:\n",
    "            density = np.concatenate((density, kde.score_samples(inputs.cpu())))\n",
    "\n",
    "    average_density = density.mean()\n",
    "    stdev_density = density.std()\n",
    "\n",
    "    average_recon_error = np.array(recon_error_list).mean()\n",
    "    stdev_recon_error = np.array(recon_error_list).std()\n",
    "\n",
    "    return (average_density, stdev_density, average_recon_error, stdev_recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train_0 = torch.where((dataset_full.targets == 0))[0]\n",
    "idx_train_2 = torch.where((dataset_full.targets == 2))[0]\n",
    "\n",
    "x_train_0 = Subset(dataset_full, idx_train_0)\n",
    "x_train_2 = Subset(dataset_full, idx_train_2)\n",
    "train_loader_0 = DataLoader(x_train_0, 2048, shuffle=False)\n",
    "train_loader_2 = DataLoader(x_train_2, 2048, shuffle=False)\n",
    "\n",
    "average_density_0, stdev_density_0, average_recon_error_0, stdev_recon_error_0 = cal_density_and_recon_error(train_loader_0, Q, P, kde)\n",
    "print(average_density_0, stdev_density_0, average_recon_error_0, stdev_recon_error_0)\n",
    "average_density_2, stdev_density_2, average_recon_error_2, stdev_recon_error_2 = cal_density_and_recon_error(train_loader_2, Q, P, kde)\n",
    "print(average_density_2, stdev_density_2, average_recon_error_2, stdev_recon_error_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density_threshold =  average_density_0 - (average_density_0 - average_density_2)/2 \n",
    "reconstruction_error_threshold = 0.03\n",
    "density_threshold = 280\n",
    "\n",
    "print(density_threshold, reconstruction_error_threshold)\n",
    "\n",
    "def check_anomaly(loader, Q:nn.Module, P:nn.Module, kde: KernelDensity, density_threshold:float, reconstruction_error_threshold:float):\n",
    "    recon_criterion = nn.MSELoss(reduction='none')\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.flatten(1).to(device)\n",
    "            z = Q(inputs)\n",
    "            reconstruction = P(z)\n",
    "            recon_error = recon_criterion(reconstruction, inputs).cpu().numpy()\n",
    "            recon_error = recon_error.mean(axis=1)\n",
    "            density = kde.score_samples(inputs.cpu())\n",
    "\n",
    "            inputs = inputs.cpu().numpy()\n",
    "            for idx in range(len(inputs)):                \n",
    "                if density[idx] < density_threshold or recon_error[idx] > reconstruction_error_threshold:\n",
    "                    result.append(2)\n",
    "                else:\n",
    "                    result.append(0)\n",
    "\n",
    "            # print(recon_error.shape)\n",
    "            # print(density.shape)\n",
    "            # print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_full = MNIST('data/', train=False, download=True, transform = transforms.ToTensor())\n",
    "idx = torch.where((test_dataset_full.targets == 0) | (test_dataset_full.targets == 2))[0]\n",
    "\n",
    "test_set = Subset(test_dataset_full, idx)\n",
    "test_loader = DataLoader(test_set, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_anomaly(test_loader, Q, P, kde, density_threshold, reconstruction_error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_set)-1)\n",
    "data, target = test_set[idx]\n",
    "plt.imshow(data[0])\n",
    "print('Target: {}, Estimation: {}'.format(target, result[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "refinement_loader =  DataLoader(x_train_set_2, 4096, shuffle=False)\n",
    "clf = svm.OneClassSVM(nu=0.02, kernel=\"rbf\", gamma=0.1)\n",
    "\n",
    "result = None\n",
    "Q.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in refinement_loader:\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "        z = Q(inputs)\n",
    "        if result is not None:\n",
    "            result = torch.cat([result, z], dim=0)\n",
    "        else:\n",
    "            result = z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(result.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = clf.predict(result.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(asd==-1)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor(np.where(asd==-1)).flatten()\n",
    "two = 0\n",
    "for i in idx:\n",
    "    if x_train_set[i][1] == 2:\n",
    "        two += 1\n",
    "\n",
    "print(two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_idx = torch.where((torch.tensor(asd) == 1))[0]\n",
    "x_train_set_2 = Subset(x_train_set, normal_idx)\n",
    "train_loader =  DataLoader(x_train_set_2, 128, shuffle=True)\n",
    "# normal_idx = torch.where((torch.tensor(asd) == 1))[0]\n",
    "# print(anomaly_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refinement epoch:500\n",
      "Refinement epoch:800\n",
      "Refinement epoch:900\n"
     ]
    }
   ],
   "source": [
    "from AnomalyDetection.AdversarialNetwork import RobustAnomalyDetector\n",
    "tb_writer = SummaryWriter('log/AAE')\n",
    "detector = RobustAnomalyDetector(32)\n",
    "detector.train(x_train_set, 128, 1000, [test_dataset_0, test_dataset_2], [500, 800, 900], tb_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
