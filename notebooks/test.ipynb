{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "ae_dir = os.path.join(project_dir, 'modules/AutoEncoder')\n",
    "if ae_dir not in sys.path:\n",
    "    sys.path.append(ae_dir)\n",
    "\n",
    "ipdl_dir = os.path.join(project_dir, 'modules/IPDL')\n",
    "if ipdl_dir not in sys.path:\n",
    "    sys.path.append(ipdl_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from autoencoder import SDAE, SDAE_TYPE\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial AutoEncoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "sdae = SDAE([28*28, 1024, 1024, z_dim], SDAE_TYPE.linear, dropout=False, \n",
    "            activation_func=[nn.ReLU(), nn.ReLU(), nn.Identity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        r'''\n",
    "            Parameters\n",
    "            ----------\n",
    "                input_dim: int\n",
    "                    Indicate the dimensionality of latent space Z\n",
    "\n",
    "                hidden_dim: int\n",
    "        '''\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D = nn.Sequential(*[\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(p=.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        ])  \n",
    "    def forward(self, x):\n",
    "        return self.D(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "Q = sdae.encode.to(device)\n",
    "\n",
    "# Decoder\n",
    "P = sdae.decode.to(device)\n",
    "\n",
    "# Discriminator\n",
    "D = Discriminator(z_dim, 512).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "dataset_full = MNIST('data/', train = True, download = True, transform = transforms.ToTensor())\n",
    "idx = torch.where((dataset_full.targets == 0) | (dataset_full.targets == 2))[0]\n",
    "\n",
    "x_train_set = Subset(dataset_full, idx)\n",
    "train_loader =  DataLoader(x_train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REMOVE! Just testing!\n",
    "x, y = next(iter(train_loader))\n",
    "plt.imshow(x[0,0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_full = MNIST('data/', train=False, download=True, transform = transforms.ToTensor())\n",
    "idx_0 = torch.where(test_dataset_full.targets == 0)[0]\n",
    "idx_2 = torch.where(test_dataset_full.targets == 2)[0]\n",
    "\n",
    "test_dataset_0 = Subset(test_dataset_full, idx_0)\n",
    "test_dataset_2 = Subset(test_dataset_full, idx_2)\n",
    "test_loader_0 =  DataLoader(test_dataset_0, 512, shuffle=False)\n",
    "test_loader_2 =  DataLoader(test_dataset_2, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining 2-Dimensional Guassian as prior\n",
    "$p(z) \\sim \\mathcal{N}_n(\\mu, \\sum)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = MultivariateNormal(loc=torch.zeros(2), covariance_matrix=torch.eye(2))\n",
    "\n",
    "plt.imshow(prior.sample((128,)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rates\n",
    "gen_lr = 1e-4\n",
    "reg_lr = 5e-5\n",
    "\n",
    "#encode/decode optimizers\n",
    "optim_P = torch.optim.Adam(P.parameters(), lr=gen_lr)\n",
    "optim_Q_enc = torch.optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "#regularizing optimizers\n",
    "optim_Q_gen = torch.optim.Adam(Q.parameters(), lr=reg_lr)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 300\n",
    "recons_criterion = nn.MSELoss()\n",
    "# recons_criterion = nn.BCELoss()\n",
    "eps = 1e-12\n",
    "tb_writer = SummaryWriter('log/AAE')\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    recons_loss_ = []\n",
    "    disc_loss_ = []\n",
    "    gen_loss_ = []\n",
    "    Q.train()\n",
    "    P.train()\n",
    "    for idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.flatten(1)\n",
    "        optim_P.zero_grad()\n",
    "        optim_Q_enc.zero_grad()\n",
    "        optim_Q_gen.zero_grad()\n",
    "        optim_D.zero_grad()\n",
    "\n",
    "        # Reconstruction loss\n",
    "        z = Q(inputs)\n",
    "        out = P(z)\n",
    "\n",
    "        recon_loss = recons_criterion(out, inputs)\n",
    "        recons_loss_.append(recon_loss.item())\n",
    "\n",
    "        recon_loss.backward()\n",
    "        optim_P.step()\n",
    "        optim_Q_enc.step()\n",
    "\n",
    "        # Discriminator\n",
    "        # Prior as an multivariate gaussian function $p(z) \\sim \\mathcal{N}_n(\\mu, \\sum)$\n",
    "        # this is constraining the Z-projection to be normal!\n",
    "        Q.eval()\n",
    "        z_real_gauss = prior.sample((inputs.size(0),)).to(device)\n",
    "        D_real_gauss = D(z_real_gauss)\n",
    "\n",
    "        z_fake_gauss = Q(inputs)\n",
    "        D_fake_gauss = D(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + eps) + torch.log(1 - D_fake_gauss + eps))\n",
    "        disc_loss_.append(D_loss.item())\n",
    "\n",
    "        D_loss.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # Generator\n",
    "        Q.train()\n",
    "        z_fake_gauss = Q(inputs)\n",
    "        D_fake_gauss = D(z_fake_gauss)\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + eps))\n",
    "        gen_loss_.append(G_loss.item())\n",
    "\n",
    "        G_loss.backward()\n",
    "        optim_Q_gen.step()\n",
    "\n",
    "    tb_writer.add_scalar('train/reconstruction', np.array(recons_loss_).mean(), epoch)\n",
    "    tb_writer.add_scalar('train/Discrimator', np.array(disc_loss_).mean(), epoch)\n",
    "    tb_writer.add_scalar('train/Generator', np.array(gen_loss_).mean(), epoch)\n",
    "    \n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_0 = []\n",
    "        loss_2 = []\n",
    "        for inputs, _ in test_loader_0:\n",
    "            inputs = inputs.to(device).flatten(1)\n",
    "            z = Q(inputs)\n",
    "            out = P(z)\n",
    "            \n",
    "            loss_0.append(recons_criterion(out, inputs).item())\n",
    "\n",
    "        for inputs, _ in test_loader_2:\n",
    "            inputs = inputs.to(device).flatten(1)\n",
    "            z = Q(inputs)\n",
    "            out = P(z)\n",
    "            \n",
    "            loss_2.append(recons_criterion(out, inputs).item())\n",
    "        \n",
    "\n",
    "        tb_writer.add_scalar('test/reconstruction/0', np.array(loss_0).mean(), epoch)\n",
    "        tb_writer.add_scalar('test/reconstruction/2',np.array(loss_2).mean(), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Q(inputs)\n",
    "out = P(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(inputs)-1)\n",
    "test_in = inputs[idx].reshape(28,28)\n",
    "test_out = out[idx].reshape(28,28)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_out.cpu().detach())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_in.cpu().detach())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "loader = DataLoader(x_train_set, 2048, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_density_and_recon_error(loader:DataLoader, Q:nn.Module, P:nn.Module, kde: KernelDensity) -> tuple:\n",
    "    density = None\n",
    "    recon_error_list = []\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    for inputs, _ in loader:\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "        z = Q(inputs)\n",
    "        reconstruction = P(z)\n",
    "        recon_error_list.append(recon_criterion(reconstruction, inputs).cpu().item())\n",
    "\n",
    "        if density is None:\n",
    "            density = kde.score_samples(inputs.cpu())\n",
    "        else:\n",
    "            density = np.concatenate((density, kde.score_samples(inputs.cpu())))\n",
    "\n",
    "    average_density = density.mean()\n",
    "    stdev_density = density.std()\n",
    "\n",
    "    average_recon_error = np.array(recon_error_list).mean()\n",
    "    stdev_recon_error = np.array(recon_error_list).std()\n",
    "\n",
    "    return (average_density, stdev_density, average_recon_error, stdev_recon_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train_0 = torch.where((dataset_full.targets == 0))[0]\n",
    "idx_train_2 = torch.where((dataset_full.targets == 2))[0]\n",
    "\n",
    "x_train_0 = Subset(dataset_full, idx_train_0)\n",
    "x_train_2 = Subset(dataset_full, idx_train_2)\n",
    "train_loader_0 = DataLoader(x_train_0, 2048, shuffle=False)\n",
    "train_loader_2 = DataLoader(x_train_2, 2048, shuffle=False)\n",
    "\n",
    "average_density_0, stdev_density_0, average_recon_error_0, stdev_recon_error_0 = cal_density_and_recon_error(train_loader_0, Q, P, kde)\n",
    "print(average_density_0, stdev_density_0, average_recon_error_0, stdev_recon_error_0)\n",
    "average_density_2, stdev_density_2, average_recon_error_2, stdev_recon_error_2 = cal_density_and_recon_error(train_loader_2, Q, P, kde)\n",
    "print(average_density_2, stdev_density_2, average_recon_error_2, stdev_recon_error_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_threshold =  average_density_0 - (average_density_0 - average_density_2)/2 \n",
    "reconstruction_error_threshold = 0.5\n",
    "\n",
    "print(density_threshold, reconstruction_error_threshold)\n",
    "\n",
    "def check_anomaly(loader, Q:nn.Module, P:nn.Module, kde: KernelDensity, density_threshold:float, reconstruction_error_threshold:float):\n",
    "    recon_criterion = nn.MSELoss(reduction='none')\n",
    "    Q.eval()\n",
    "    P.eval()\n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.flatten(1).to(device)\n",
    "            z = Q(inputs)\n",
    "            reconstruction = P(z)\n",
    "            recon_error = recon_criterion(reconstruction, inputs).cpu().numpy()\n",
    "            recon_error = recon_error.mean(axis=1)\n",
    "            density = kde.score_samples(inputs.cpu())\n",
    "\n",
    "            inputs = inputs.cpu().numpy()\n",
    "            for idx in range(len(inputs)):                \n",
    "                if density[idx] < density_threshold or recon_error[idx] > reconstruction_error_threshold:\n",
    "                    result.append(2)\n",
    "                else:\n",
    "                    result.append(0)\n",
    "\n",
    "            # print(recon_error.shape)\n",
    "            # print(density.shape)\n",
    "            # print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_full = MNIST('data/', train=False, download=True, transform = transforms.ToTensor())\n",
    "idx = torch.where((test_dataset_full.targets == 0) | (test_dataset_full.targets == 2))[0]\n",
    "\n",
    "test_set = Subset(test_dataset_full, idx)\n",
    "test_loader = DataLoader(test_set, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = check_anomaly(test_loader, Q, P, kde, density_threshold, reconstruction_error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_set)-1)\n",
    "data, target = test_set[idx]\n",
    "plt.imshow(data[0])\n",
    "print('Target: {}, Estimation: {}'.format(target, result[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
